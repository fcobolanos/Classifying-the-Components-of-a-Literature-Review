{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQPiXgRnS66Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/MyDrive/olmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5URp347LSvvh"
      },
      "outputs": [],
      "source": [
        "!pip install \"torch==2.5.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATm-jkTD2vHp"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes peft trl datasets evaluate #ai2-olmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaYx0U-d25sx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data_type='Original'\n",
        "#data_type='Synthetic'\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_excel('LR_Dataset_Original_Sythetic_Final.xlsx')\n",
        "#df = pd.read_excel('LR_Dataset_Original_Sythetic_Experiment_70.xlsx') # 20% of the training and validation sets\n",
        "\n",
        "\n",
        "if data_type=='Original':\n",
        " df = df[df['Source'] == 'Original'] # Only original\n",
        "\n",
        "df= df[['Sentence','Category' ,'Classification']]\n",
        "\n",
        "df['Sentence'] = df['Sentence'].str.capitalize()\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "df['Sentence'] = (df['Sentence']\n",
        "                  .str.strip()\n",
        "                  .str.replace(r'\\n|\\r', ' ', regex=True)\n",
        "                  .str.replace(r'\\s{2,}', ' ', regex=True))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg7G_WGN66BM"
      },
      "outputs": [],
      "source": [
        "def create_instruction(row):\n",
        "    sentence = row['Sentence']\n",
        "    instruction = (\n",
        "                \"You are a researcher that should assign a classification to a sentence from scientific articles, choosing from one of the following seven categories. Each category corresponds to a specific aspect of scientific discourse, either related to a topic or a study. A topic is defined as a scientific domain, such as “Computer Science” or “Machine  Learning”. A previous study refers to a prior paper on the topic.\\n\"\n",
        "                \"Categories:\\n \"\n",
        "                \"1. OVERALL: Describes, introduces, classifies, or defines research topics often based on the discussion of multiple previous studies together.\\n \"\n",
        "                \"2. RESEARCH GAP: Highlights the need for further research within the topic.\\n\"\n",
        "                \"3. DESCRIPTION: Outlines the objectives, methodology, or design of one previous study, without mentioning results.\\n\"\n",
        "                \"4. RESULT: Describes specific findings or outcomes drawn from previous studies. This category includes empirical results, theoretical insights, and observed patterns reported by researchers. It often uses verbs like “showed”, “found”, “demonstrated”, and “observed” or phrases like “the findings indicate”.\\n\"\n",
        "                \"5. LIMITATION: Describes a constraint, challenge, or weakness inherent in the methodology of a previous study that hinders generalizability or reliability in a previous study.\\n\"\n",
        "                \"6. EXTENSION: Describes how the current study addresses or extends previous studies by stating the overall idea, contrasting ideas or elaborating further ideas. It usually uses the words “we” or “our”.\\n\"\n",
        "                \"7. OTHER: Any text that does not fit the above categories.\\n\"\n",
        "                \"Procedure:\\n\"\n",
        "                \"1. Determine whether the subject of the sentence is a topic or a study.\\n\"\n",
        "                \"2. Identify the most suitable category based on the content. Do not create new categories. Use the categories given above.\\n\"\n",
        "                \"3. Provide the category number that best fits the sentence. Just provide the category number without any explanation.\\n\"\n",
        "\n",
        "                f\"Sentence: {sentence}.\\n\"\n",
        "            )\n",
        "\n",
        "    return instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VHBIP04Stib"
      },
      "outputs": [],
      "source": [
        "df['instruction'] = df.apply(create_instruction, axis=1)\n",
        "\n",
        "df = df.rename(columns={'Category': 'response'})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Ax_3Ym5Gef"
      },
      "outputs": [],
      "source": [
        "test_dataset =df[df['Classification'] == 'TEST']\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhGUFWz05OXV"
      },
      "outputs": [],
      "source": [
        "def generate_test_prompt(example):\n",
        "    \"\"\"Format prompt for training.\"\"\"\n",
        "    text = f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Bzlml4TBOE"
      },
      "outputs": [],
      "source": [
        "test_dataset['text']=test_dataset.apply(generate_test_prompt, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrkSIopQ5aQl"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1rPyluV3E8G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,pipeline\n",
        "\n",
        "# Model Setup (Quantization and Tokenizer)\n",
        "\n",
        "#model_name=\"hamishivi/OLMo-1B-0724-Instruct-hf\"\n",
        "model_name=\"allenai/OLMo-7B-0724-Instruct-hf\"\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    #bnb_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config, # Comment for hamishivi/OLMo-1B-0724-Instruct-hf\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    #torch_dtype=torch.float16, # This new #torch.bfloat16\n",
        ")\n",
        "\n",
        "# Tokenizer Initialization\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                                          #torch_dtype=torch.float16, # This new #torch.bfloat16\n",
        "                                          #device_map=\"auto\", # Added\n",
        "                                          #trust_remote_code=True,)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpFkaCgH6e-z"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"olmo_1B_Instruct-lro-finetune/checkpoint-1225\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjW_dH74Zxj"
      },
      "outputs": [],
      "source": [
        "def extract_text(text):\n",
        "    # Define both markers to search for\n",
        "    markers = [\"<|im_end|>assistant\", \"assistant\\n\"]\n",
        "\n",
        "    # Loop through markers and check if each is in the text\n",
        "    for marker in markers:\n",
        "        marker_position = text.find(marker)\n",
        "\n",
        "        # If the marker is found, extract text after it\n",
        "        if marker_position != -1:\n",
        "            return text[marker_position + len(marker):].strip()  # Remove any leading/trailing whitespace\n",
        "\n",
        "    # Return None if neither marker is found\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0izh_0To9U_s"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    # Check for keywords and replace text accordingly\n",
        "    if \"OVERALL\" in text:\n",
        "        return \"OVERALL\"\n",
        "    if \"RESEARCH GAP\" in text:\n",
        "        return \"RESEARCH GAP\"\n",
        "    if \"DESCRIPTION\" in text:\n",
        "        return \"DESCRIPTION\"\n",
        "    if \"RESULT\" in text:\n",
        "        return \"RESULT\"\n",
        "    if \"LIMITATION\" in text:\n",
        "        return \"LIMITATION\"\n",
        "    if \"EXTENSION\" in text:\n",
        "        return \"EXTENSION\"\n",
        "    elif \"OTHER\" in text:\n",
        "        return \"OTHER\"\n",
        "\n",
        "    # If neither keyword is found, return the original text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdDumyHp3-HN"
      },
      "outputs": [],
      "source": [
        "prompt=test_dataset.iloc[0]['text']\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I__sZU-e_STE"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "outputs = ft_model.generate(**inputs, max_new_tokens=15, use_cache=True)\n",
        "answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebZC8rHkrIkc"
      },
      "outputs": [],
      "source": [
        "clean_text(extract_text(answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzkGp3xw4uG3"
      },
      "outputs": [],
      "source": [
        "def get_classification_finetuning(data_point,model,tokenizer):\n",
        "    \"\"\"\n",
        "    Gets the classification for a data point using the fine-tuned model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(data_point['text'],return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=15, use_cache=True)\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    print(clean_text(extract_text(answer)))\n",
        "    data_point['Prediction_Finetune']=answer  # Assign the result to the data point\n",
        "    data_point['Prediction_Finetune_Cleaned']=clean_text(extract_text(answer))\n",
        "\n",
        "    return data_point\n",
        "# Apply the get_classification function to the dataset using map\n",
        "test_dataset = test_dataset.apply(lambda row: get_classification_finetuning(row, ft_model, tokenizer), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTpzY1jmaR_Z"
      },
      "outputs": [],
      "source": [
        "test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogZurCzG4FBM"
      },
      "outputs": [],
      "source": [
        "test_dataset.to_csv('OLMo_FT_Test_LoRA_Augmented_Final.csv', index=False)\n",
        "#test_dataset.to_csv('SmolLM2_FT_Test_NEFT_Augmented.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}