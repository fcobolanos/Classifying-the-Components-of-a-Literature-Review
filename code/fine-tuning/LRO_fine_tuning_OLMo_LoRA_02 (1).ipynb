{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vS8CAL7-raJS",
      "metadata": {
        "id": "vS8CAL7-raJS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/MyDrive/olmo\n",
        "# original:%cd drive/MyDrive/mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4d1464",
      "metadata": {
        "id": "0d4d1464"
      },
      "outputs": [],
      "source": [
        "!pip install \"torch==2.5.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0fb496",
      "metadata": {
        "id": "8d0fb496"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes peft trl datasets evaluate #ai2-olmo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "rOuENZqPgcKv"
      },
      "id": "rOuENZqPgcKv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d55154a",
      "metadata": {
        "id": "7d55154a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig,TrainingArguments\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pmyDSsQu-nrf",
      "metadata": {
        "id": "pmyDSsQu-nrf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "#data_type='Original'\n",
        "data_type='Synthetic'\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_excel('LR_Dataset_Original_Sythetic_Final.xlsx')\n",
        "#df = pd.read_excel('LR_Dataset_Original_Sythetic_Experiment_70.xlsx') # 20% of the training and validation sets\n",
        "\n",
        "\n",
        "if data_type=='Original':\n",
        " df = df[df['Source'] == 'Original'] # Only original\n",
        "\n",
        "df= df[['Sentence','Category' ,'Classification']]\n",
        "\n",
        "df['Sentence'] = df['Sentence'].str.capitalize()\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "df['Sentence'] = (df['Sentence']\n",
        "                  .str.strip()\n",
        "                  .str.replace(r'\\n|\\r', ' ', regex=True)\n",
        "                  .str.replace(r'\\s{2,}', ' ', regex=True))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3_2LjOGJzlIO",
      "metadata": {
        "id": "3_2LjOGJzlIO"
      },
      "outputs": [],
      "source": [
        "def create_instruction(row):\n",
        "    sentence = row['Sentence']\n",
        "    instruction = (\n",
        "                \"You are a researcher that should assign a classification to a sentence from scientific articles, choosing from one of the following seven categories. Each category corresponds to a specific aspect of scientific discourse, either related to a topic or a study. A topic is defined as a scientific domain, such as “Computer Science” or “Machine  Learning”. A previous study refers to a prior paper on the topic.\\n\"\n",
        "                \"Categories:\\n \"\n",
        "                \"1. OVERALL: Describes, introduces, classifies, or defines research topics often based on the discussion of multiple previous studies together.\\n \"\n",
        "                \"2. RESEARCH GAP: Highlights the need for further research within the topic.\\n\"\n",
        "                \"3. DESCRIPTION: Outlines the objectives, methodology, or design of one previous study, without mentioning results.\\n\"\n",
        "                \"4. RESULT: Describes specific findings or outcomes drawn from previous studies. This category includes empirical results, theoretical insights, and observed patterns reported by researchers. It often uses verbs like “showed”, “found”, “demonstrated”, and “observed” or phrases like “the findings indicate”.\\n\"\n",
        "                \"5. LIMITATION: Describes a constraint, challenge, or weakness inherent in the methodology of a previous study that hinders generalizability or reliability in a previous study.\\n\"\n",
        "                \"6. EXTENSION: Describes how the current study addresses or extends previous studies by stating the overall idea, contrasting ideas or elaborating further ideas. It usually uses the words “we” or “our”.\\n\"\n",
        "                \"7. OTHER: Any text that does not fit the above categories.\\n\"\n",
        "                \"Procedure:\\n\"\n",
        "                \"1. Determine whether the subject of the sentence is a topic or a study.\\n\"\n",
        "                \"2. Identify the most suitable category based on the content. Do not create new categories. Use the categories given above.\\n\"\n",
        "                \"3. Provide the category number that best fits the sentence. Just provide the category number without any explanation.\\n\"\n",
        "\n",
        "                f\"Sentence: {sentence}.\\n\"\n",
        "            )\n",
        "\n",
        "    return instruction\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['instruction'] = df.apply(create_instruction, axis=1)\n",
        "\n",
        "df = df.rename(columns={'Category': 'response'})\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ZDmnbgx0PAvu"
      },
      "id": "ZDmnbgx0PAvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = df[df['Classification'] == 'TRAINING']\n",
        "train_dataset= train_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "eval_dataset = df[df['Classification'] == 'VALIDATION']\n",
        "eval_dataset= eval_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_dataset =df[df['Classification'] == 'TEST']\n",
        "test_dataset= test_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "3gH5vNsmPGuF"
      },
      "id": "3gH5vNsmPGuF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "Uxoc3qFgPLh1"
      },
      "id": "Uxoc3qFgPLh1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xakStVDPlbLy",
      "metadata": {
        "id": "xakStVDPlbLy"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(example):\n",
        "    \"\"\"Format prompt for training.\"\"\"\n",
        "    text = f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\\n<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
        "    return text\n",
        "\n",
        "def generate_test_prompt(example):\n",
        "    \"\"\"Format prompt for training.\"\"\"\n",
        "    text = f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\"\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XZEQYvfgzI6q",
      "metadata": {
        "id": "XZEQYvfgzI6q"
      },
      "outputs": [],
      "source": [
        "train_dataset['text']= train_dataset.apply(generate_prompt, axis=1)\n",
        "eval_dataset['text']= eval_dataset.apply(generate_prompt, axis=1)\n",
        "test_dataset['text']=test_dataset.apply(generate_test_prompt, axis=1)\n",
        "\n",
        "train_data = Dataset.from_pandas(train_dataset[['text']])\n",
        "eval_data = Dataset.from_pandas(eval_dataset[['text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXqfecf31iit",
      "metadata": {
        "id": "fXqfecf31iit"
      },
      "outputs": [],
      "source": [
        "eval_data\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ski59FE0_ke6",
      "metadata": {
        "id": "ski59FE0_ke6"
      },
      "outputs": [],
      "source": [
        "#train_data['text'][0]\n",
        "#eval_data\n",
        "eval_data['text'][33]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d40c969",
      "metadata": {
        "id": "8d40c969"
      },
      "outputs": [],
      "source": [
        "#model_name=\"hamishivi/OLMo-1B-0724-Instruct-hf\"\n",
        "model_name=\"allenai/OLMo-7B-0724-Instruct-hf\" # OLMo_FT_Test_NEST_2.csv\n",
        "#model_name=\"allenai/OLMo-7B-Instruct-hf\"\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    #bnb_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config, # Comment for hamishivi/OLMo-1B-0724-Instruct-hf\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    #torch_dtype=torch.float16, # This new #torch.bfloat16\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "ojWexgK8pEOF"
      },
      "id": "ojWexgK8pEOF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "N42gYga_jvfZ"
      },
      "id": "N42gYga_jvfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C1QEYjTUDxnf",
      "metadata": {
        "id": "C1QEYjTUDxnf"
      },
      "outputs": [],
      "source": [
        "prompt=test_dataset['text'].iloc[0]\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False)\n",
        "inputs = {k: v.to('cuda') for k,v in inputs.items()}\n",
        "#response = model.generate(**inputs, max_new_tokens=20, do_sample=True, top_k=50, top_p=0.95)\n",
        "response = model.generate(**inputs, max_new_tokens=20, use_cache=True)\n",
        "\n",
        "\n",
        "answer=tokenizer.batch_decode(response, skip_special_tokens=True)[0]\n",
        "answer"
      ],
      "metadata": {
        "id": "hnmbZau7kR74"
      },
      "id": "hnmbZau7kR74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef3b342",
      "metadata": {
        "id": "1ef3b342"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16, # 256  Original 16\n",
        "    lora_alpha=32, #128 Original 32\n",
        "    target_modules=[\n",
        "        \"self_attn.q_proj\",\n",
        "        \"self_attn.k_proj\",\n",
        "        \"self_attn.v_proj\",\n",
        "        \"self_attn.o_proj\",\n",
        "        \"mlp.gate_proj\",\n",
        "        \"mlp.up_proj\",\n",
        "        \"mlp.down_proj\",\n",
        "        #\"mlp.lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.1,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Project and Model Setup\n",
        "project = \"lro-finetune\"\n",
        "base_model_name = \"olmo_1B_Instruct\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,# Orginal 4\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2, # 4\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=0,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True, # Original =True\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",  # Save the model every epoch\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,\n",
        "    max_seq_length=1024,\n",
        "    #neftune_noise_alpha=5 # Comment out for NEFT.\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8bde541",
      "metadata": {
        "id": "f8bde541"
      },
      "source": [
        "The following code will train the model using the trainer.train() method and then save the trained model to the trained-model directory. Using The standard GPU P100 offered by Kaggle, the training should be quite fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfeb8e7c",
      "metadata": {
        "id": "dfeb8e7c"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "DKqdJAk9viBb"
      },
      "id": "DKqdJAk9viBb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f3b18181",
      "metadata": {
        "id": "f3b18181"
      },
      "source": [
        "Afterwards, loading the TensorBoard extension and start TensorBoard, pointing to the logs/runs directory, which is assumed to contain the training logs and checkpoints for your model, will allow you to understand how the models fits during the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r6i7jJUNK7pF",
      "metadata": {
        "id": "r6i7jJUNK7pF"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "#del [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\n",
        "#del [df, train_dataset, eval_dataset]\n",
        "#del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FeWgL32jK_qO",
      "metadata": {
        "id": "FeWgL32jK_qO"
      },
      "outputs": [],
      "source": [
        "for _ in range(100):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9qrw8aOLFjx",
      "metadata": {
        "id": "b9qrw8aOLFjx"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8v89A_ZcLHN2",
      "metadata": {
        "id": "8v89A_ZcLHN2"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "finetuned_model = \"./smollm2_7B_Instruct-lro-finetune/\"\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "     finetuned_model,\n",
        "     torch_dtype=compute_dtype,\n",
        "     return_dict=False,\n",
        "     low_cpu_mem_usage=True,\n",
        "     device_map=device,\n",
        ")\n",
        "\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\n",
        "tokenizer.save_pretrained(\"./merged_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IqGoGyfNS_Mv",
      "metadata": {
        "id": "IqGoGyfNS_Mv"
      },
      "outputs": [],
      "source": [
        "prompt=test_dataset['text'].iloc[0]\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EjuEWObALRwT",
      "metadata": {
        "id": "EjuEWObALRwT"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(task=\"text-generation\",\n",
        "                        model=merged_model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens = 100,\n",
        "                        temperature = 0.0,\n",
        "                       )\n",
        "result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "answer = result[0]['generated_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12OURNWQzdIk",
      "metadata": {
        "id": "12OURNWQzdIk"
      },
      "outputs": [],
      "source": [
        "def extract_text(text):\n",
        "    # Define both markers to search for\n",
        "    markers = [\"<|im_end|>assistant\", \"assistant\\n\"]\n",
        "\n",
        "    # Loop through markers and check if each is in the text\n",
        "    for marker in markers:\n",
        "        marker_position = text.find(marker)\n",
        "\n",
        "        # If the marker is found, extract text after it\n",
        "        if marker_position != -1:\n",
        "            return text[marker_position + len(marker):].strip()  # Remove any leading/trailing whitespace\n",
        "\n",
        "    # Return None if neither marker is found\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fWmfmtj7wfMa",
      "metadata": {
        "id": "fWmfmtj7wfMa"
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wXh0fJ54TR0R",
      "metadata": {
        "id": "wXh0fJ54TR0R"
      },
      "outputs": [],
      "source": [
        "extract_text(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0o0GcE-Tefd",
      "metadata": {
        "id": "a0o0GcE-Tefd"
      },
      "outputs": [],
      "source": [
        "# Getting the Classification\n",
        "def get_classification_finetuning(data_point,merged_model,tokenizer):\n",
        "    \"\"\"\n",
        "    Gets the classification for a data point using the fine-tuned model.\n",
        "    \"\"\"\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                        model=merged_model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens = 100,\n",
        "                        temperature = 0.0,\n",
        "                       )\n",
        "    result = pipe(data_point['text'], pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "    answer = result[0]['generated_text']\n",
        "    print(f\"Sentence : {data_point.index[-1]}\")\n",
        "    print(answer)\n",
        "    #answer = answer.split(\"=\")[-1].lower()\n",
        "    data_point['Prediction_Finetune']=answer  # Assign the result to the data point\n",
        "    data_point['Prediction_Finetune_Cleaned']=extract_text(answer)\n",
        "\n",
        "    return data_point\n",
        "\n",
        "# Apply the get_classification function to the dataset using map\n",
        "test_dataset = test_dataset.apply(lambda row: get_classification_finetuning(row, merged_model, tokenizer), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fOOIGk0nb6rN",
      "metadata": {
        "id": "fOOIGk0nb6rN"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35bc9852",
      "metadata": {
        "id": "35bc9852"
      },
      "outputs": [],
      "source": [
        "test_dataset.to_csv('SmolLM2_FT_Test_LoRA2.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 622510,
          "sourceId": 1192499,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4357.442516,
      "end_time": "2024-03-15T16:00:37.581641",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-03-15T14:48:00.139125",
      "version": "2.5.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}