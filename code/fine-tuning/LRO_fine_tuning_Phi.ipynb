{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Dependencies"
      ],
      "metadata": {
        "id": "IqM-T1RTzY6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install unsloth\n",
        "%pip install pandas\n",
        "# Also get the latest nightly Unsloth!\n",
        "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ],
      "metadata": {
        "id": "5Lr7G2YXzuT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "WgjEZyo7Ncf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall transformers -y\n",
        "#!pip install --upgrade --no-cache-dir \"git+https://github.com/huggingface/transformers.git\""
      ],
      "metadata": {
        "id": "c4YJV_hcR3t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive"
      ],
      "metadata": {
        "id": "r2v_X2fA0Df5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/MyDrive/phi_unsloth_1"
      ],
      "metadata": {
        "id": "UYpqhHdLNHTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Path"
      ],
      "metadata": {
        "id": "M0mESZUfNJCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model_name = \"unsloth/Phi-3.5-mini-instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "SXd9bTZd1aaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256, #Original: 16, # Best: 256\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\", ], # New:  \"lora_magnitude_vector\"\n",
        "    lora_alpha = 128, # Original # Best: 128\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    #use_dora=True # Comment it out\n",
        "    #lora_query=True, # Comment it out not the original setting.\n",
        "    #lora_value=True # Comment it out not the original setting.\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preparation"
      ],
      "metadata": {
        "id": "vITh0KVJ10qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "# Load Data\n",
        "df = pd.read_excel('LR_Dataset_Original_Sythetic_Final.xlsx')\n",
        "df= df[['Sentence','Category' ,'Classification']]\n",
        "\n",
        "\n",
        "df['Sentence'] = df['Sentence'].str.capitalize()\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "df['Sentence'] = (df['Sentence']\n",
        "                  .str.strip()\n",
        "                  .str.replace(r'\\n|\\r', ' ', regex=True)\n",
        "                  .str.replace(r'\\s{2,}', ' ', regex=True))\n",
        "df\n"
      ],
      "metadata": {
        "id": "x-rg0HMGNty5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df =df[df['Classification'] == 'TRAINING']\n",
        "#train_df =train_df.shuffle(seed=42)\n",
        "val_df =df[df['Classification'] == 'VALIDATION']\n",
        "#eval_df =eval_df.shuffle(seed=42)\n",
        "test_df = df[df['Classification'] == 'TEST']\n",
        "#test_df =test_df.shuffle(seed=42)"
      ],
      "metadata": {
        "id": "eEPZzz0I_pWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "FnkjCR34NvUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head(1))"
      ],
      "metadata": {
        "id": "DOuZyWRFhPwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conversation(row):\n",
        "    sentence = row['Sentence']\n",
        "    answer = row['Category']\n",
        "\n",
        "    human = (\n",
        "                \"You are a researcher that should assign a classification to a sentence from scientific articles, choosing from one of the following seven categories. Each category corresponds to a specific aspect of scientific discourse, either related to a topic or a study. A topic is defined as a scientific domain, such as “Computer Science” or “Machine  Learning”. A previous study refers to a prior paper on the topic.\\n\"\n",
        "                \"Categories:\\n \"\n",
        "                \"1. OVERALL: Describes, introduces, classifies, or defines research topics often based on the discussion of multiple previous studies together.\\n \"\n",
        "                \"2. RESEARCH GAP: Highlights the need for further research within the topic.\\n\"\n",
        "                \"3. DESCRIPTION: Outlines the objectives, methodology, or design of one previous study, without mentioning results.\\n\"\n",
        "                \"4. RESULT: Describes specific findings or outcomes drawn from previous studies. This category includes empirical results, theoretical insights, and observed patterns reported by researchers. It often uses verbs like “showed”, “found”, “demonstrated”, and “observed” or phrases like “the findings indicate”.\\n\"\n",
        "                \"5. LIMITATION: Describes a constraint, challenge, or weakness inherent in the methodology of a previous study that hinders generalizability or reliability in a previous study.\\n\"\n",
        "                \"6. EXTENSION: Describes how the current study addresses or extends previous studies by stating the overall idea, contrasting ideas or elaborating further ideas. It usually uses the words “we” or “our”.\\n\"\n",
        "                \"7. OTHER: Any text that does not fit the above categories.\\n\"\n",
        "                \"Procedure:\\n\"\n",
        "                \"1. Determine whether the subject of the setence is a topic or a study.\\n\"\n",
        "                \"2. Identify the most suitable category based on the content. Do not create new categories. Use the categories given above.\\n\"\n",
        "                \"3. Provide the category number that best fits the sentence. Just provide the category number without any explanation.\\n\"\n",
        "\n",
        "                f\"Sentence: {sentence}.\\n\"\n",
        "            )\n",
        "\n",
        "    gpt = f\"Classification: {answer}\"\n",
        "\n",
        "\n",
        "\n",
        "    return [\n",
        "        {\"from\": \"human\", \"value\": human},\n",
        "        {\"from\": \"gpt\", \"value\": gpt},\n",
        "    ]"
      ],
      "metadata": {
        "id": "Oeen7r-Y8dsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conversation_test(row):\n",
        "    sentence = row['Sentence']\n",
        "\n",
        "\n",
        "    human = (\n",
        "                \"You are a researcher that should assign a classification to a sentence from scientific articles, choosing from one of the following seven categories. Each category corresponds to a specific aspect of scientific discourse, either related to a topic or a study. A topic is defined as a scientific domain, such as “Computer Science” or “Machine  Learning”. A previous study refers to a prior paper on the topic.\\n\"\n",
        "                \"Categories:\\n \"\n",
        "                \"1. OVERALL: Describes, introduces, classifies, or defines research topics often based on the discussion of multiple previous studies together.\\n \"\n",
        "                \"2. RESEARCH GAP: Highlights the need for further research within the topic.\\n\"\n",
        "                \"3. DESCRIPTION: Outlines the objectives, methodology, or design of one previous study, without mentioning results.\\n\"\n",
        "                \"4. RESULT: Describes specific findings or outcomes drawn from previous studies. This category includes empirical results, theoretical insights, and observed patterns reported by researchers. It often uses verbs like “showed”, “found”, “demonstrated”, and “observed” or phrases like “the findings indicate”.\\n\"\n",
        "                \"5. LIMITATION: Describes a constraint, challenge, or weakness inherent in the methodology of a previous study that hinders generalizability or reliability in a previous study.\\n\"\n",
        "                \"6. EXTENSION: Describes how the current study addresses or extends previous studies by stating the overall idea, contrasting ideas or elaborating further ideas. It usually uses the words “we” or “our”.\\n\"\n",
        "                \"7. OTHER: Any text that does not fit the above categories.\\n\"\n",
        "                \"Procedure:\\n\"\n",
        "                \"1. Determine whether the subject of the setence is a topic or a study.\\n\"\n",
        "                \"2. Identify the most suitable category based on the content. Do not create new categories. Use the categories given above.\\n\"\n",
        "                \"3. Provide the category number that best fits the sentence. Just provide the category number without any explanation.\\n\"\n",
        "\n",
        "                f\"Sentence: {sentence}.\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "    return [\n",
        "        {\"from\": \"human\", \"value\": human},\n",
        "\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "uPvrMgGb8jsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['conversations'] = train_df.apply(create_conversation, axis=1)\n",
        "val_df['conversations'] = val_df.apply(create_conversation, axis=1)\n",
        "test_df['conversations'] = test_df.apply(create_conversation_test, axis=1)"
      ],
      "metadata": {
        "id": "gGoAV7lP8n9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['conversations']\n",
        "#val_df['conversations']\n",
        "#test_df['conversations']\n"
      ],
      "metadata": {
        "id": "5gX4CeAMGSw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ],
      "metadata": {
        "id": "DjMULtEf8ryw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)\n",
        "val_dataset = val_dataset.map(formatting_prompts_func, batched = True,)\n",
        "test_dataset = test_dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "2xWLvaRgPUk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[100][\"text\"])"
      ],
      "metadata": {
        "id": "kVnfuf9RQyLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "# Project and Model Setup\n",
        "project = \"lro-finetune\"\n",
        "base_model_name = \"Phi-Instruct\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,  # Numero di valutazioni senza miglioramenti prima di fermare\n",
        "    early_stopping_threshold=0.01  # Soglia di miglioramento minimo\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8,\n",
        "        per_device_eval_batch_size = 8,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # Orginal 60\n",
        "        evaluation_strategy = \"steps\",  # Can also be \"epoch\"\n",
        "        eval_steps = 10,  # Evaluate every 10 steps\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,# Original: 0.01\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        output_dir = output_dir,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        "    callbacks = [early_stopping_callback] # Delete the , and the code below for original setting.\n",
        "    #neftune_noise_alpha=5   # Add noise to embeddings\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the model"
      ],
      "metadata": {
        "id": "9O57FRO8RefG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "GEaf68ECReFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_converted = test_dataset.to_pandas()\n",
        "test_df_converted"
      ],
      "metadata": {
        "id": "Z3c64Pw-cofJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=test_df_converted['text'].loc[13]\n",
        "prompt\n"
      ],
      "metadata": {
        "id": "-oAxSRO2LOjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(prompt,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "answer"
      ],
      "metadata": {
        "id": "ULJsSA0pHkcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_text(text):\n",
        "    # Extract text after \"Classification: \"\n",
        "    match = re.search(r\"Classification:\\s*(.*)\", text)\n",
        "    if match:\n",
        "        classification = match.group(1)\n",
        "        # Remove extra spaces and convert to uppercase\n",
        "        cleaned_classification = ' '.join(classification.split()).strip().upper()\n",
        "        return cleaned_classification\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "YQ6F0yZ6z-2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_text(answer)"
      ],
      "metadata": {
        "id": "JQbT75sp0Dj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Classification\n",
        "def get_classification(data_point,model,tokenizer):\n",
        "    \"\"\"\n",
        "    Gets the classification for a data point using the fine-tuned model.\n",
        "    \"\"\"\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "                data_point['text'],\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    print(f\"Sentence : {data_point.index[-1]}\")\n",
        "    print(answer)\n",
        "    data_point['Prediction_Finetune']=answer\n",
        "    data_point['Prediction_Finetune_Clean']=extract_text(answer)\n",
        "\n",
        "\n",
        "\n",
        "    return data_point\n",
        "\n",
        "# Apply the get_classification function to the dataset using map\n",
        "test_df_converted = test_df_converted.apply(lambda row: get_classification(row, model, tokenizer), axis=1)"
      ],
      "metadata": {
        "id": "VrNFdWYmJXcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_converted"
      ],
      "metadata": {
        "id": "1WSGgdBEK_OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_converted.to_csv('Mistral7B_Nemo_FT_Test_Change2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "nU6lrxM-LCgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make Inference"
      ],
      "metadata": {
        "id": "ekOmTR1hSNcr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
