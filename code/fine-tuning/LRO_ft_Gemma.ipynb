{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "#### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ],
      "metadata": {
        "id": "7UQh528-UXC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall transformers -y\n",
        "#!pip install --upgrade --no-cache-dir \"git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "SiCKTPB0dgyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "#### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYpqhHdLNHTW"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/MyDrive/gemma_unsloth_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0mESZUfNJCl"
      },
      "source": [
        "#### Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7cjv4UxCsve"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"GemmaInstruct-lro-finetune\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit\n",
        ")\n",
        "\n",
        "# Prepare the model for inference\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-rg0HMGNty5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_excel('LR_Dataset_Original_Sythetic_Final.xlsx')\n",
        "df= df[['Sentence','Category' ,'Classification']]\n",
        "\n",
        "\n",
        "df['Sentence'] = df['Sentence'].str.capitalize()\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "df['Sentence'] = (df['Sentence']\n",
        "                  .str.strip()\n",
        "                  .str.replace(r'\\n|\\r', ' ', regex=True)\n",
        "                  .str.replace(r'\\s{2,}', ' ', regex=True))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEPZzz0I_pWi"
      },
      "outputs": [],
      "source": [
        "test_df = df[df['Classification'] == 'TEST']\n",
        "#test_df =test_df.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnkjCR34NvUB"
      },
      "outputs": [],
      "source": [
        "print(test_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOuZyWRFhPwB"
      },
      "outputs": [],
      "source": [
        "print(test_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPvrMgGb8jsw"
      },
      "outputs": [],
      "source": [
        "def create_conversation_test(row):\n",
        "    sentence = row['Sentence']\n",
        "\n",
        "\n",
        "    human = (\n",
        "                \"You are a researcher that should assign a classification to a sentence from scientific articles, choosing from one of the following seven categories. Each category corresponds to a specific aspect of scientific discourse, either related to a topic or a study. A topic is defined as a scientific domain, such as “Computer Science” or “Machine  Learning”. A previous study refers to a prior paper on the topic.\\n\"\n",
        "                \"Categories:\\n \"\n",
        "                \"1. OVERALL: Describes, introduces, classifies, or defines research topics often based on the discussion of multiple previous studies together.\\n \"\n",
        "                \"2. RESEARCH GAP: Highlights the need for further research within the topic.\\n\"\n",
        "                \"3. DESCRIPTION: Outlines the objectives, methodology, or design of one previous study, without mentioning results.\\n\"\n",
        "                \"4. RESULT: Describes specific findings or outcomes drawn from previous studies. This category includes empirical results, theoretical insights, and observed patterns reported by researchers. It often uses verbs like “showed”, “found”, “demonstrated”, and “observed” or phrases like “the findings indicate”.\\n\"\n",
        "                \"5. LIMITATION: Describes a constraint, challenge, or weakness inherent in the methodology of a previous study that hinders generalizability or reliability in a previous study.\\n\"\n",
        "                \"6. EXTENSION: Describes how the current study addresses or extends previous studies by stating the overall idea, contrasting ideas or elaborating further ideas. It usually uses the words “we” or “our”.\\n\"\n",
        "                \"7. OTHER: Any text that does not fit the above categories.\\n\"\n",
        "                \"Procedure:\\n\"\n",
        "                \"1. Determine whether the subject of the setence is a topic or a study.\\n\"\n",
        "                \"2. Identify the most suitable category based on the content. Do not create new categories. Use the categories given above.\\n\"\n",
        "                \"3. Provide the category number that best fits the sentence. Just provide the category number without any explanation.\\n\"\n",
        "\n",
        "                f\"Sentence: {sentence}.\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "    return [\n",
        "        {\"from\": \"human\", \"value\": human},\n",
        "\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGoAV7lP8n9I"
      },
      "outputs": [],
      "source": [
        "test_df['conversations'] = test_df.apply(create_conversation_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gX4CeAMGSw-"
      },
      "outputs": [],
      "source": [
        "test_df['conversations']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjMULtEf8ryw"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xWLvaRgPUk9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_dataset = test_dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veDJ8UA8EcEM"
      },
      "outputs": [],
      "source": [
        "test_df_converted = test_dataset.to_pandas()\n",
        "test_df_converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oAxSRO2LOjD"
      },
      "outputs": [],
      "source": [
        "prompt=test_df_converted['text'].loc[4]\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULJsSA0pHkcE"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(prompt,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
        "answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ6F0yZ6z-2Y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_text(text):\n",
        "    # Extract text after \"Classification: \"\n",
        "    match = re.search(r\"Classification:\\s*(.*)\", text)\n",
        "    if match:\n",
        "        classification = match.group(1)\n",
        "        # Remove extra spaces and convert to uppercase\n",
        "        cleaned_classification = ' '.join(classification.split()).strip().upper()\n",
        "        return cleaned_classification\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQbT75sp0Dj5"
      },
      "outputs": [],
      "source": [
        "extract_text(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrNFdWYmJXcx"
      },
      "outputs": [],
      "source": [
        "# Getting the Classification\n",
        "def get_classification(data_point,model,tokenizer):\n",
        "    \"\"\"\n",
        "    Gets the classification for a data point using the fine-tuned model.\n",
        "    \"\"\"\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "                data_point['text'],\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    print(f\"Sentence : {data_point.index[-1]}\")\n",
        "    print(answer)\n",
        "    data_point['Prediction_Finetune']=answer\n",
        "    data_point['Prediction_Finetune_Clean']=extract_text(answer)\n",
        "\n",
        "\n",
        "\n",
        "    return data_point\n",
        "\n",
        "# Apply the get_classification function to the dataset using map\n",
        "test_df_converted = test_df_converted.apply(lambda row: get_classification(row, model, tokenizer), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WSGgdBEK_OQ"
      },
      "outputs": [],
      "source": [
        "test_df_converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU6lrxM-LCgC"
      },
      "outputs": [],
      "source": [
        "#test_df_converted.to_csv('Gemma_Instruct_FT_Test_NEFT_Augmented1.csv', index=False)\n",
        "test_df_converted.to_csv('Gemma_Instruct_FT_Test_LoRA_Augmented1.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "#### Make Inference"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
